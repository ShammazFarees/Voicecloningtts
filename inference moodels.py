# -*- coding: utf-8 -*-
"""Models Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qv8-7VtGRq7FsgmoyjacxGMxTvHmHkpl

**Setting Up the environment**
"""

!pip install git+https://github.com/coqui-ai/TTS.git

!pip install --upgrade pandas==2.2.2

!pip install transformers datasets accelerate librosa torchaudio wandb
!pip install onnxruntime

!pip uninstall -y numpy
!pip install numpy --upgrade --force-reinstall
!pip install --upgrade --force-reinstall soundfile datasets scipy

!pip install numpy==1.24.4

# Full Fix for NumPy Compatibility Issues
!pip uninstall -y numpy
!pip install numpy==1.26.4 --force-reinstall

!pip install torchaudio

"""**Inference & testing Each Model**

1.   **Kokoro-82M:** A lightweight 82-million parameter transformer model designed for efficient and fast inference on small devices or edge applications.
2. **CSM-1B:** A 1-billion parameter conversational AI model optimized for natural, context-aware dialogue and instruction following.
3. **XTTS-v2:** A multilingual, multi-speaker text-to-speech model capable of generating realistic speech in various languages and voices from text.
"""

!git clone https://github.com/hexgrad/kokoro.git
!cd kokoro

# 1️⃣ Install required packages
!pip install -q kokoro>=0.9.4 soundfile ipython
!apt-get -qq -y install espeak-ng > /dev/null 2>&1

# 2️⃣ Initialize the pipeline with enhanced configuration
from kokoro import KPipeline
from IPython.display import display, Audio
import soundfile as sf
import torch
from typing import Optional

class KokoroTTS:
    def __init__(self, lang_code: str = 'a', device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        """
        Initialize Kokoro TTS pipeline with automatic device detection

        Args:
            lang_code: Language code ('a'=English, 'j'=Japanese, etc.)
            device: 'cuda' or 'cpu'
        """
        self.pipeline = KPipeline(lang_code=lang_code)
        self.device = device

    def synthesize(
        self,
        text: str,
        voice: str = 'af_heart',
        speed: float = 1.0,
        voice_tensor: Optional[torch.Tensor] = None,
        split_pattern: str = r'\n+',
        save_path: Optional[str] = None
    ) -> tuple:
        """
        Generate speech with voice cloning capabilities

        Args:
            text: Input text to synthesize
            voice: Predefined voice name
            speed: Playback speed (0.5-2.0)
            voice_tensor: Custom voice tensor for cloning
            split_pattern: Regex for text splitting
            save_path: Optional path to save audio

        Returns:
            tuple: (graphemes, phonemes, audio_array)
        """
        try:
            generator = self.pipeline(
                text,
                voice=voice_tensor if voice_tensor else voice,
                speed=speed,
                split_pattern=split_pattern
            )

            results = []
            for i, (gs, ps, audio) in enumerate(generator):
                print(f"Segment {i+1}:")
                print(f"Text: {gs}")
                print(f"Phonemes: {ps}")

                # Display in notebook
                display(Audio(data=audio, rate=24000, autoplay=i==0))

                # Save if path provided
                if save_path:
                    segment_path = f"{save_path}_{i}.wav" if save_path else None
                    sf.write(segment_path, audio, 24000)
                    print(f"Saved to {segment_path}")

                results.append((gs, ps, audio))

            return results

        except Exception as e:
            print(f"Error during synthesis: {str(e)}")
            return None

# 3️⃣ Example Usage
if __name__ == "__main__":
    # Initialize with American English
    tts = KokoroTTS(lang_code='a')

    # Sample text with multiple paragraphs
    sample_text = """
    Kokoro is an advanced text-to-speech system that combines efficiency with high-quality output.
    The model supports voice cloning and emotional inflection, making it suitable for various applications.

    With just 82 million parameters, it achieves performance comparable to larger models while being significantly faster.
    The Apache license allows for both research and commercial use.
    """

    # Generate with default voice
    print("\nGenerating with default voice...")
    default_results = tts.synthesize(
        text=sample_text,
        voice='af_heart',
        speed=1.1,
        save_path='default_voice'
    )

    # Voice cloning example (requires pre-loaded voice tensor)
    # voice_tensor = torch.load('custom_voice.pt', weights_only=True)
    # print("\nGenerating with cloned voice...")
    # cloned_results = tts.synthesize(
    #     text=sample_text,
    #     voice_tensor=voice_tensor,
    #     speed=0.9,
    #     save_path='cloned_voice'
    # )

!pip install transformers huggingface-hub

from huggingface_hub import login
# For regular Python scripts:
login()

# Commented out IPython magic to ensure Python compatibility.
!git clone git@github.com:SesameAILabs/csm.git
# %cd csm

!pip install -r requirements.txt

import sys
sys.path.append("/content/csm")  # Add repo to Python path

from generator import load_csm_1b
import torchaudio
import torch

# Set device (MPS for Apple Silicon, CUDA for NVIDIA, else CPU)
device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"

# Load model
generator = load_csm_1b(device=device)

# Generate speech
audio = generator.generate(
    text="Hello from Sesame!",
    speaker=0,  # Check repo for speaker IDs
    context=[],  # Optional context (if supported)
    max_audio_length_ms=10_000,  # 10 seconds
)

# Save and play audio
torchaudio.save("audio.wav", audio.unsqueeze(0).cpu(), generator.sample_rate)

# Play in Colab
import IPython.display as ipd
ipd.Audio("audio.wav")

texts = [
    "The quick brown fox jumps over the lazy dog.",
    "Artificial intelligence is transforming the world.",
    "How much wood would a woodchuck chuck if a woodchuck could chuck wood?",
    "Sesame Street is a beloved children's television show.",
]

for i, text in enumerate(texts):
    print(f"Generating audio for: '{text}'")
    audio = generator.generate(
        text=text,
        speaker=0,  # Default speaker
        context=[],
        max_audio_length_ms=10_000,
    )
    torchaudio.save(f"audio_{i}.wav", audio.unsqueeze(0).cpu(), generator.sample_rate)
    display(ipd.Audio(f"audio_{i}.wav"))

# Install TTS with pip
!pip install TTS

from TTS.api import TTS

# Initialize the TTS with explicit parameters
tts = TTS(model_name="xtts_v2")

# Run TTS with required parameters
tts.tts_to_file(
    text="Hello world! This is a test of the I+XTTS-v2 model.",
    file_path="output.wav",
    speaker_wav="/content/audio.wav",  # Path to a voice sample
    language="en"  # Required language code
)